import pandas as pd
import glob
import os
import numpy as np
from datetime import datetime, timedelta, date
import warnings
from pandas.errors import SettingWithCopyWarning
from dateutil.relativedelta import relativedelta
import calendar

warnings.filterwarnings("ignore", category=SettingWithCopyWarning)

# ==================== é…ç½®åŒº ====================
EXCHANGE_RATE = 281
SEARCH_PATH = "."
OUTPUT_FILENAME = 'æœ€ç»ˆç”Ÿæˆä¸‰ç»´åº¦æŠ¥è¡¨.xlsx'
CSV_ENCODINGS = ['utf-8-sig', 'utf-8', 'gbk', 'latin1', 'ISO-8859-1', 'cp1252']
SPEND_SHEET_URL = "https://docs.google.com/spreadsheets/d/1v_GQLJ28byHFUqfcvdo2OtAbLQaMOCBL9x7Q-MqkjmI/export?format=csv"
OVERRIDE_DATE = "2025-10-5"

# ==================== è¾“å‡ºå­—æ®µå®šä¹‰ ====================
# åŸæœ‰å­—æ®µ
PRODUCT_COLS = [
    'æ—¥æœŸ', 'äº§å“', 'æ€»æ¶ˆè€—(U)', 'æ³¨å†Œæˆæœ¬', 'é¦–å……æˆæœ¬', 'æ–°å¢ç”¨æˆ·æ•°', 'å……å€¼äººæ•°', 'å……å€¼é‡‘é¢(U)', 'æç°é‡‘é¢(U)',
    'å……å‡æ(U)', 'ç›ˆä½™ç‡(%)', 'é¦–å­˜äººæ•°', 'é¦–å­˜å……å€¼é‡‘é¢(U)', 'é¦–å­˜ä»˜è´¹ç‡(%)',
    'é¦–å­˜ARPPU', 'æ–°å¢ä»˜è´¹äººæ•°', 'æ–°å¢å……å€¼é‡‘é¢(U)', 'æ–°å¢ä»˜è´¹ç‡(%)', 'è€ç”¨æˆ·å……å€¼äººæ•°',
    'è€ç”¨æˆ·å……å€¼é‡‘é¢(U)', 'è€ç”¨æˆ·ä»˜è´¹ç‡(%)', 'è€ç”¨æˆ·ARPPU', 'è€ç”¨æˆ·ç›ˆä½™ç‡(%)', 'ARPPU',
    'æ¬¡æ—¥å¤å……ç‡(%)', '3æ—¥å¤å……ç‡(%)', '7æ—¥å¤å……ç‡(%)', '15æ—¥å¤å……ç‡(%)', '30æ—¥å¤å……ç‡(%)',
    'LTV-7å¤©', 'LTV-15å¤©', 'LTV-30å¤©', 'è£‚å˜ç‡'
]
DEPT_COLS = [
    'æ—¥æœŸ', 'éƒ¨é—¨', 'æ€»æ¶ˆè€—(U)', 'æ³¨å†Œæˆæœ¬', 'é¦–å……æˆæœ¬', 'æ–°å¢ç”¨æˆ·æ•°', 'å……å€¼äººæ•°', 'å……å€¼é‡‘é¢(U)', 'æç°é‡‘é¢(U)',
    'å……å‡æ(U)', 'ç›ˆä½™ç‡(%)', 'é¦–å­˜äººæ•°', 'é¦–å­˜å……å€¼é‡‘é¢(U)', 'é¦–å­˜ä»˜è´¹ç‡(%)',
    'é¦–å­˜ARPPU', 'æ–°å¢ä»˜è´¹äººæ•°', 'æ–°å¢å……å€¼é‡‘é¢(U)', 'æ–°å¢ä»˜è´¹ç‡(%)', 'è€ç”¨æˆ·å……å€¼äººæ•°',
    'è€ç”¨æˆ·å……å€¼é‡‘é¢(U)', 'è€ç”¨æˆ·ä»˜è´¹ç‡(%)', 'è€ç”¨æˆ·ARPPU', 'è€ç”¨æˆ·ç›ˆä½™ç‡(%)', 'ARPPU',
    'æ¬¡æ—¥å¤å……ç‡(%)', '3æ—¥å¤å……ç‡(%)', '7æ—¥å¤å……ç‡(%)', '15æ—¥å¤å……ç‡(%)', '30æ—¥å¤å……ç‡(%)',
    'LTV-7å¤©', 'LTV-15å¤©', 'LTV-30å¤©', 'è€ç”¨æˆ·å……å‡æ(U)', 'è€ç©å®¶æ—¥æ´»'
]
CHANNEL_COLS = [
    'æ—¥æœŸ', 'éƒ¨é—¨', 'æ¸ é“æ¥æº', 'æ€»æ¶ˆè€—(U)', 'æ³¨å†Œæˆæœ¬', 'é¦–å……æˆæœ¬', 'æ–°å¢ç”¨æˆ·æ•°', 'å……å€¼äººæ•°', 'å……å€¼é‡‘é¢(U)',
    'æç°é‡‘é¢(U)', 'å……å‡æ(U)', 'ç›ˆä½™ç‡(%)', 'é¦–å­˜äººæ•°', 'é¦–å­˜å……å€¼é‡‘é¢(U)', 'é¦–å­˜ä»˜è´¹ç‡(%)',
    'é¦–å­˜ARPPU', 'æ–°å¢ä»˜è´¹äººæ•°', 'æ–°å¢å……å€¼é‡‘é¢(U)', 'æ–°å¢ä»˜è´¹ç‡(%)', 'è€ç”¨æˆ·å……å€¼äººæ•°',
    'è€ç”¨æˆ·å……å€¼é‡‘é¢(U)', 'è€ç”¨æˆ·ä»˜è´¹ç‡(%)', 'è€ç”¨æˆ·ARPPU', 'è€ç”¨æˆ·ç›ˆä½™ç‡(%)', 'ARPPU',
    'æ¬¡æ—¥å¤å……ç‡(%)', '3æ—¥å¤å……ç‡(%)', '7æ—¥å¤å……ç‡(%)', '15æ—¥å¤å……ç‡(%)', '30æ—¥å¤å……ç‡(%)',
    'LTV-7å¤©', 'LTV-15å¤©', 'LTV-30å¤©', 'è€ç”¨æˆ·å……å‡æ(U)', 'è€ç©å®¶æ—¥æ´»'
]

# ä¸ºæ–°çš„å‘¨æŠ¥å’ŒæœˆæŠ¥è®¾è®¡çš„æ‰©å±•å­—æ®µ
PRODUCT_REPORT_COLS = [
    'å‘¨æœŸ', 'å§‹æ—¥-æœ«æœŸ', 'æ€»æ¶ˆè€—(U)', 'æ³¨å†Œæˆæœ¬', 'é¦–å……æˆæœ¬', 'æ–°å¢ç”¨æˆ·æ•°', 'å……å€¼äººæ•°', 'å……å€¼é‡‘é¢(U)', 'æç°é‡‘é¢(U)',
    'å……å‡æ(U)', 'ç›ˆä½™ç‡(%)', 'é¦–å­˜äººæ•°', 'é¦–å­˜å……å€¼é‡‘é¢(U)', 'é¦–å­˜ä»˜è´¹ç‡(%)', 'é¦–å­˜ç›ˆä½™ç‡(%)', 'é¦–å­˜ARPPU',
    'æ–°å¢ä»˜è´¹äººæ•°', 'æ–°å¢å……å€¼é‡‘é¢(U)', 'æ–°å¢ä»˜è´¹ç‡(%)', 'è€ç”¨æˆ·å……å€¼äººæ•°', 'è€ç”¨æˆ·å……å€¼é‡‘é¢(U)',
    'è€ç”¨æˆ·ä»˜è´¹ç‡(%)', 'è€ç”¨æˆ·ARPPU', 'è€ç”¨æˆ·ç›ˆä½™ç‡(%)', 'ARPPU', 'æ¬¡æ—¥å¤å……ç‡(%)', '3æ—¥å¤å……ç‡(%)',
    '7æ—¥å¤å……ç‡(%)', '15æ—¥å¤å……ç‡(%)', '30æ—¥å¤å……ç‡(%)', 'LTV-7å¤©', 'LTV-15å¤©', 'LTV-30å¤©', 'è£‚å˜ç‡',
    'è€ç”¨æˆ·å……å‡æ(U)', 'è€ç©å®¶æ—¥æ´»', 'å†å²æ¶ˆè€—', 'å†å²å……æå·®'
]
PRODUCT_MONTHLY_COLS_PREFIX = ['å¼€å§‹', 'ç»“æŸ']
PRODUCT_MONTHLY_REPORT_COLS = PRODUCT_MONTHLY_COLS_PREFIX + PRODUCT_REPORT_COLS


# ==================== æ ¸å¿ƒæ•°æ®å¤„ç†å‡½æ•° ====================
def map_and_aggregate(df, df_mapping, value_cols, weight_col):
    if df is None or df.empty or df_mapping is None: return pd.DataFrame()
    mapping_dict = {row.iloc[1]: row.iloc[0] for _, row in df_mapping.iterrows()}
    def map_channel_to_dept(channel):
        for keyword, dept in mapping_dict.items():
            if keyword in str(channel): return dept
        return 'unknown'
    df['éƒ¨é—¨'] = df.iloc[:, 1].apply(map_channel_to_dept)

    unmapped_channels = df[df['éƒ¨é—¨'] == 'unknown'].iloc[:, 1].unique()
    if len(unmapped_channels) > 0:
        print("\n" + "="*20 + " [é…ç½®è­¦å‘Š] " + "="*20)
        print(f"  ä»¥ä¸‹æ¸ é“/æ¥æºåœ¨'æ€»ä»£åç§°éƒ¨é—¨è§„å¾‹è¡¨.csv'ä¸­æ‰¾ä¸åˆ°åŒ¹é…çš„éƒ¨é—¨ï¼Œå°†è¢«å½’å…¥'unknown'ï¼š")
        for channel in unmapped_channels:
            print(f"   - {channel}")
        print("  è¿™å¯èƒ½å¯¼è‡´ã€éƒ¨é—¨-æ—¥æŠ¥ã€‘å’Œã€æ¸ é“-æ—¥æŠ¥ã€‘ä¸­çš„å¤å……ç‡ç­‰æ•°æ®ä¸º0ã€‚")
        print("  è¯·æ£€æŸ¥æ‚¨çš„æ˜ å°„è¡¨ï¼Œä¸ºè¿™äº›æ¸ é“/æ¥æºæ·»åŠ æ­£ç¡®çš„éƒ¨é—¨æ˜ å°„ã€‚")
        print("="*65 + "\n")

    for col in value_cols:
        if col in df.columns:
            df[f'weighted_{col}'] = pd.to_numeric(df[col], errors='coerce').fillna(0) * pd.to_numeric(df[weight_col], errors='coerce').fillna(0)
    agg_dict = {f'weighted_{col}': 'sum' for col in value_cols if f'weighted_{col}' in df.columns}
    agg_dict[weight_col] = 'sum'
    dept_summary = df.groupby('éƒ¨é—¨').agg(agg_dict).reset_index()
    for col in value_cols:
        if f'weighted_{col}' in dept_summary.columns and dept_summary[weight_col].sum() > 0:
            avg_rate = dept_summary[f'weighted_{col}'] / dept_summary[weight_col].replace(0, np.nan)
            dept_summary[col] = avg_rate / 100.0
    return dept_summary[['éƒ¨é—¨'] + value_cols].fillna(0)

def read_and_process_recharge_data(df_recharge_source, df_mapping, report_date):
    print("--- æ­£åœ¨ä¸ºæ—¥æŠ¥å¤„ç†ã€é¦–å……ç”¨æˆ·åˆ†æã€‘æ•°æ® ---")
    if df_recharge_source is None or df_recharge_source.empty: return pd.DataFrame(), pd.Series(dtype=float)

    df_recharge = df_recharge_source.copy()
    if 'é¦–å……äººæ•°' in df_recharge.columns:
        df_recharge.rename(columns={'é¦–å……äººæ•°': 'é¦–å­˜äººæ•°'}, inplace=True)

    rate_map = {'æ¬¡æ—¥å¤å……ç‡(%)': 1, '3æ—¥å¤å……ç‡(%)': 3, '7æ—¥å¤å……ç‡(%)': 7, '15æ—¥å¤å……ç‡(%)': 15, '30æ—¥å¤å……ç‡(%)': 30}
    all_dept_rates = []
    total_rates = {}

    for rate_col, days_back in rate_map.items():
        if rate_col not in df_recharge.columns: continue

        target_date = report_date - timedelta(days=days_back)
        cohort_data = df_recharge[df_recharge['æ—¥æœŸ'] == target_date].copy()

        if not cohort_data.empty:
            cohort_data[rate_col] = pd.to_numeric(cohort_data[rate_col], errors='coerce').fillna(0)
            cohort_data['é¦–å­˜äººæ•°'] = pd.to_numeric(cohort_data['é¦–å­˜äººæ•°'], errors='coerce').fillna(0)

            weighted_sum = (cohort_data[rate_col] * cohort_data['é¦–å­˜äººæ•°']).sum()
            total_users = cohort_data['é¦–å­˜äººæ•°'].sum()
            avg_rate = weighted_sum / total_users if total_users > 0 else 0
            total_rates[rate_col] = avg_rate / 100.0

            dept_rates = map_and_aggregate(cohort_data, df_mapping, [rate_col], 'é¦–å­˜äººæ•°')
            all_dept_rates.append(dept_rates)
        else:
            total_rates[rate_col] = 0

    if not all_dept_rates:
        return pd.DataFrame(), pd.Series(total_rates)

    final_dept_df = all_dept_rates[0]
    for df in all_dept_rates[1:]:
        if not df.empty: final_dept_df = pd.merge(final_dept_df, df, on='éƒ¨é—¨', how='outer')

    print("--- æ—¥æŠ¥å¤å……ç‡æ•°æ®å¤„ç†å®Œæˆ ---\n")
    return final_dept_df.fillna(0), pd.Series(total_rates)


# ==================== è¾…åŠ©å‡½æ•° ====================
def write_sheet_with_metadata(df, sheet_name, writer, period_type, exchange_rate):
    workbook = writer.book; worksheet = workbook.add_worksheet(sheet_name); writer.sheets[sheet_name] = worksheet
    report_date = date.today().strftime('%Y-%m-%d')
    if 'æ—¥æœŸ' in df.columns and not df['æ—¥æœŸ'].dropna().empty:
        valid_dates = pd.to_datetime(df['æ—¥æœŸ'].dropna(), errors='coerce').dropna()
        if not valid_dates.empty: report_date = valid_dates.max().strftime('%Y-%m-%d')

    header_format = workbook.add_format({'align': 'left', 'bold': True, 'font_size': 11})
    if period_type in ['æ—¥æŠ¥', 'å‘¨æŠ¥æ€»æ±‡', 'æœˆæŠ¥æ€»æ±‡']:
        worksheet.merge_range('B1:F1', 'äº§å“ï¼šaa-å·´åŸºæ–¯å¦ï¼ˆEpiWinï¼‰', header_format)
        if period_type == 'æ—¥æŠ¥':
             worksheet.write('H1', f"æ—¥æŠ¥æ—¥æœŸ: {report_date}", header_format)
    else:
         worksheet.merge_range('B1:F1', f'äº§å“å‘¨æŠ¥æ€»æ±‡' if 'å‘¨æŠ¥' in sheet_name else 'äº§å“æœˆæŠ¥æ€»æ±‡', header_format)

    worksheet.merge_range('B3:G3', f'{sheet_name.split("-")[0]}æ€»æ±‡è¡¨', header_format)
    df.to_excel(writer, sheet_name=sheet_name, index=False, startrow=3)

def read_source_file(file_pattern):
    files = glob.glob(os.path.join(SEARCH_PATH, file_pattern))
    if not files: print(f"  [è­¦å‘Š] æœªæ‰¾åˆ°æ–‡ä»¶: {file_pattern}"); return None
    latest_file = max(files, key=os.path.getctime); print(f"  æ­£åœ¨è¯»å–: {os.path.basename(latest_file)}")
    df = None
    try:
        is_excel = False
        if latest_file.endswith('.csv'):
            with open(latest_file, 'rb') as f:
                try:
                    if f.read(4) == b'PK\x03\x04': is_excel = True
                except: pass
        if is_excel or latest_file.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(latest_file)
        elif latest_file.endswith('.csv'):
            for encoding in CSV_ENCODINGS:
                try:
                    df = pd.read_csv(latest_file, encoding=encoding, sep=None, header=0, on_bad_lines='skip', engine='python')
                    if df.shape[1] > 1: print(f"  [è¯Šæ–­] ä½¿ç”¨'{encoding}'ç¼–ç æˆåŠŸè§£æä¸ºCSVã€‚"); break
                    else: df = None
                except: continue
        if df is not None:
            df.columns = df.columns.str.strip(); print(f"  [æˆåŠŸ] æ–‡ä»¶å·²è¯»å–å¹¶æ¸…ç†åˆ—åã€‚")
            return df
        else: print(f"  [é”™è¯¯] æ— æ³•æ­£ç¡®è¯»å–æ–‡ä»¶: {latest_file}"); return None
    except Exception as e: print(f"  [é”™è¯¯] è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"); return None

def get_spend_data_from_google_sheet(url):
    print("æ­£åœ¨ä»Google Sheeté“¾æ¥è¯»å–'æ¶ˆè€—'æ•°æ®...")
    try:
        df = pd.read_csv(url); df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], format='%Y%m%d').dt.date
        spend_cols = [col for col in df.columns if col not in ['æ—¥æœŸ', 'å’Œ', 'å¤‡æ³¨']]
        for col in spend_cols: df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        df['æ€»æ¶ˆè€—(U)'] = df[spend_cols].sum(axis=1); return df[['æ—¥æœŸ', 'æ€»æ¶ˆè€—(U)']]
    except Exception as e: print(f"  [é”™è¯¯] è‡ªåŠ¨è¯»å–Google Sheetå¤±è´¥: {e}"); return None

def calculate_metrics(df):
    df = df.copy()
    currency_cols = ['å……å€¼é‡‘é¢', 'æç°é‡‘é¢', 'é¦–å­˜å……å€¼é‡‘é¢', 'è€ç”¨æˆ·å……å€¼é‡‘é¢', 'æ–°å¢å……å€¼é‡‘é¢', 'è€ç”¨æˆ·æç°é‡‘é¢']
    for col in currency_cols:
        if col in df.columns: df[f'{col}(U)'] = pd.to_numeric(df[col], errors='coerce').fillna(0) / EXCHANGE_RATE
    with np.errstate(divide='ignore', invalid='ignore'):
        df['å……å‡æ(U)'] = df.get('å……å€¼é‡‘é¢(U)', 0) - df.get('æç°é‡‘é¢(U)', 0)
        df['è€ç”¨æˆ·å……å‡æ(U)'] = df.get('è€ç”¨æˆ·å……å€¼é‡‘é¢(U)', 0) - df.get('è€ç”¨æˆ·æç°é‡‘é¢(U)', 0)
        cols_to_ensure = ['æ€»æ¶ˆè€—(U)', 'æ–°å¢ç”¨æˆ·æ•°', 'é¦–å­˜äººæ•°', 'è€ç”¨æˆ·å……å€¼äººæ•°', 'è€ç©å®¶æ—¥æ´»', 'å……å€¼äººæ•°', 'æ–°å¢ä»˜è´¹äººæ•°']
        for col in cols_to_ensure:
            if col not in df.columns: df[col] = 0
            else: df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        df['æ³¨å†Œæˆæœ¬'] = df['æ€»æ¶ˆè€—(U)'] / df['æ–°å¢ç”¨æˆ·æ•°']; df['é¦–å……æˆæœ¬'] = df['æ€»æ¶ˆè€—(U)'] / df['é¦–å­˜äººæ•°']
        df['ç›ˆä½™ç‡(%)'] = df['å……å‡æ(U)'] / df['å……å€¼é‡‘é¢(U)']; df['è€ç”¨æˆ·ç›ˆä½™ç‡(%)'] = df['è€ç”¨æˆ·å……å‡æ(U)'] / df['è€ç”¨æˆ·å……å€¼é‡‘é¢(U)']

        df['è€ç”¨æˆ·ä»˜è´¹ç‡(%)'] = df['è€ç”¨æˆ·å……å€¼äººæ•°'] / df['è€ç©å®¶æ—¥æ´»']
        df['é¦–å­˜ä»˜è´¹ç‡(%)'] = df['é¦–å­˜äººæ•°'] / df['æ–°å¢ç”¨æˆ·æ•°']
        df['æ–°å¢ä»˜è´¹ç‡(%)'] = df['æ–°å¢ä»˜è´¹äººæ•°'] / df['æ–°å¢ç”¨æˆ·æ•°']; df['é¦–å­˜ARPPU'] = df['é¦–å­˜å……å€¼é‡‘é¢(U)'] / df['é¦–å­˜äººæ•°']
        df['è€ç”¨æˆ·ARPPU'] = df['è€ç”¨æˆ·å……å€¼é‡‘é¢(U)'] / df['è€ç”¨æˆ·å……å€¼äººæ•°']; df['ARPPU'] = df['å……å€¼é‡‘é¢(U)'] / df['å……å€¼äººæ•°']
        df['é¦–å­˜ç›ˆä½™ç‡(%)'] = 0
    df.replace([np.inf, -np.inf], np.nan, inplace=True); df.fillna(0, inplace=True)
    return df

# ==================== æ–°å¢/é‡æ„çš„æŠ¥è¡¨ç”Ÿæˆå‡½æ•° ====================

def calculate_period_repurchase_rates(df_recharge_source, period_start_date, period_end_date):
    if df_recharge_source is None or df_recharge_source.empty: return {}
    df_recharge = df_recharge_source.copy()
    if 'é¦–å……äººæ•°' in df_recharge.columns: df_recharge.rename(columns={'é¦–å……äººæ•°': 'é¦–å­˜äººæ•°'}, inplace=True)
    if 'æ—¥æœŸ' not in df_recharge.columns or 'é¦–å­˜äººæ•°' not in df_recharge.columns: return {}

    rate_map = {'æ¬¡æ—¥å¤å……ç‡(%)': 1, '3æ—¥å¤å……ç‡(%)': 3, '7æ—¥å¤å……ç‡(%)': 7, '15æ—¥å¤å……ç‡(%)': 15, '30æ—¥å¤å……ç‡(%)': 30}
    calculated_rates = {}

    for rate_col, _ in rate_map.items():
        if rate_col not in df_recharge.columns: continue
        period_cohort_data = df_recharge[
            (df_recharge['æ—¥æœŸ'] >= period_start_date) & (df_recharge['æ—¥æœŸ'] <= period_end_date)
        ].copy()
        if period_cohort_data.empty:
            calculated_rates[rate_col] = 0; continue
        period_cohort_data['é¦–å­˜äººæ•°'] = pd.to_numeric(period_cohort_data['é¦–å­˜äººæ•°'], errors='coerce').fillna(0)
        period_cohort_data[rate_col] = pd.to_numeric(period_cohort_data[rate_col], errors='coerce').fillna(0)
        period_cohort_data['weighted_rate'] = period_cohort_data[rate_col] * period_cohort_data['é¦–å­˜äººæ•°']
        total_weighted_rate = period_cohort_data['weighted_rate'].sum()
        total_weight = period_cohort_data['é¦–å­˜äººæ•°'].sum()
        avg_rate = total_weighted_rate / total_weight if total_weight > 0 else 0
        calculated_rates[rate_col] = avg_rate / 100.0
    return calculated_rates

def aggregate_and_calculate_period(df_period, all_historical_data, start_date, end_date, df_recharge_source):
    if df_period.empty: return pd.DataFrame()
    df_summary = pd.DataFrame(df_period.sum(numeric_only=True)).T
    df_summary = calculate_metrics(df_summary)

    for days in [7, 15, 30]:
        window_start_date = start_date; window_end_date = start_date + timedelta(days=days-1)
        window_df = all_historical_data[
            (all_historical_data['æ—¥æœŸ'] >= window_start_date) & (all_historical_data['æ—¥æœŸ'] <= window_end_date) &
            (all_historical_data['æ—¥æœŸ'] <= end_date)
        ]
        total_recharge = window_df['å……å€¼é‡‘é¢(U)'].sum()
        total_new_users = df_period['æ–°å¢ç”¨æˆ·æ•°'].sum()
        df_summary[f'LTV-{days}å¤©'] = total_recharge / total_new_users if total_new_users else 0

    history_to_end = all_historical_data[all_historical_data['æ—¥æœŸ'] <= end_date]
    df_summary['å†å²æ¶ˆè€—'] = history_to_end['æ€»æ¶ˆè€—(U)'].sum()
    df_summary['å†å²å……æå·®'] = history_to_end['å……å‡æ(U)'].sum()

    period_rates = calculate_period_repurchase_rates(df_recharge_source, start_date, end_date)
    if period_rates:
        for col, value in period_rates.items():
            df_summary[col] = value
    return df_summary

def generate_product_weekly_report(all_historical_data, report_date, writer, df_recharge_source):
    print("--- æ­£åœ¨ç”Ÿæˆã€äº§å“-å‘¨æŠ¥æ€»æ±‡ã€‘ ---")
    this_week_start = report_date - timedelta(days=report_date.weekday())
    this_week_end = this_week_start + timedelta(days=6)
    last_week_start = this_week_start - timedelta(days=7)
    last_week_end = this_week_start - timedelta(days=1)

    df_this_week = all_historical_data[(all_historical_data['æ—¥æœŸ'] >= this_week_start) & (all_historical_data['æ—¥æœŸ'] <= this_week_end)]
    df_last_week = all_historical_data[(all_historical_data['æ—¥æœŸ'] >= last_week_start) & (all_historical_data['æ—¥æœŸ'] <= last_week_end)]

    row_this_week = aggregate_and_calculate_period(df_this_week, all_historical_data, this_week_start, this_week_end, df_recharge_source)
    row_last_week = aggregate_and_calculate_period(df_last_week, all_historical_data, last_week_start, last_week_end, df_recharge_source)

    if row_this_week.empty: row_this_week = pd.DataFrame(columns=row_last_week.columns if not row_last_week.empty else PRODUCT_REPORT_COLS, index=[0]).fillna(0)
    if row_last_week.empty: row_last_week = pd.DataFrame(columns=row_this_week.columns, index=[0]).fillna(0)

    row_diff = row_this_week.iloc[0] - row_last_week.iloc[0]
    with np.errstate(divide='ignore', invalid='ignore'):
        row_wow = (row_this_week.iloc[0] - row_last_week.iloc[0]) / row_last_week.iloc[0].replace(0, np.nan)

    df_final = pd.DataFrame(columns=PRODUCT_REPORT_COLS)
    data_map = {'ä¸Šå‘¨': row_last_week.iloc[0], 'æœ¬å‘¨': row_this_week.iloc[0], 'æ•°å·®': row_diff, 'ç¯æ¯”': row_wow}
    date_ranges = {
        'ä¸Šå‘¨': f"{last_week_start.strftime('%m/%d')}-{last_week_end.strftime('%m/%d')}",
        'æœ¬å‘¨': f"{this_week_start.strftime('%m/%d')}-{this_week_end.strftime('%m/%d')}",
        'æ•°å·®': 'æœ¬å‘¨-ä¸Šå‘¨', 'ç¯æ¯”': 'æœ¬å‘¨/ä¸Šå‘¨-1'
    }
    for period, data_row in data_map.items():
        new_row = {'å‘¨æœŸ': period, 'å§‹æ—¥-æœ«æœŸ': date_ranges[period]}
        for col in PRODUCT_REPORT_COLS:
            if col in data_row.index: new_row[col] = data_row[col]
        df_final = pd.concat([df_final, pd.DataFrame([new_row])], ignore_index=True)

    for col in df_final.columns:
        if '(%)' in col or 'ç‡' in col or col == 'ç¯æ¯”':
            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').apply(lambda x: f"{x:.2%}" if pd.notna(x) else 'N/A')
    write_sheet_with_metadata(df_final[PRODUCT_REPORT_COLS], 'äº§å“-å‘¨æŠ¥æ€»æ±‡', writer, 'å‘¨æŠ¥', EXCHANGE_RATE)


def generate_product_monthly_report(all_historical_data, report_date, writer, df_recharge_source):
    print("--- æ­£åœ¨ç”Ÿæˆã€äº§å“-æœˆæŠ¥æ€»æ±‡ã€‘ ---")
    report_list = []

    for i in range(4):
        target_month_date = report_date - relativedelta(months=i)
        start_of_month = target_month_date.replace(day=1)
        end_of_month = target_month_date.replace(day=calendar.monthrange(target_month_date.year, target_month_date.month)[1])
        df_month = all_historical_data[(all_historical_data['æ—¥æœŸ'] >= start_of_month) & (all_historical_data['æ—¥æœŸ'] <= end_of_month)]
        if df_month.empty: continue
        row_month = aggregate_and_calculate_period(df_month, all_historical_data, start_of_month, end_of_month, df_recharge_source)
        row_month['å¼€å§‹'] = start_of_month.strftime('%Y/%m/%d'); row_month['ç»“æŸ'] = end_of_month.strftime('%Y/%m/%d')
        row_month['å‘¨æœŸ'] = f"{start_of_month.month}æœˆ"; row_month['å§‹æ—¥-æœ«æœŸ'] = f"{start_of_month.month}æœˆ"
        report_list.append(row_month)

    for i in range(2):
        target_month_date = report_date - relativedelta(months=i)
        start_of_month = target_month_date.replace(day=1)
        _, days_in_month = calendar.monthrange(target_month_date.year, target_month_date.month)
        for week_num in range(1, 6):
            start_of_week = start_of_month + timedelta(days=(week_num-1)*7)
            end_of_week = start_of_week + timedelta(days=6)
            if start_of_week.month != target_month_date.month or start_of_week.day > days_in_month: break
            df_week = all_historical_data[(all_historical_data['æ—¥æœŸ'] >= start_of_week) & (all_historical_data['æ—¥æœŸ'] <= end_of_week)]
            if df_week.empty: continue
            row_week = aggregate_and_calculate_period(df_week, all_historical_data, start_of_week, end_of_week, df_recharge_source)
            row_week['å¼€å§‹'] = start_of_week.strftime('%Y/%m/%d'); row_week['ç»“æŸ'] = end_of_week.strftime('%Y/%m/%d')
            row_week['å‘¨æœŸ'] = f"ç¬¬{week_num}å‘¨"; row_week['å§‹æ—¥-æœ«æœŸ'] = f"{start_of_week.strftime('%m/%d')}-{end_of_week.strftime('%m/%d')}"
            report_list.append(row_week)

    if not report_list: print("  [è­¦å‘Š] æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”ŸæˆæœˆæŠ¥ã€‚"); return
    df_final = pd.concat(report_list, ignore_index=True)

    this_month_data = df_final[df_final['å‘¨æœŸ'] == f"{report_date.month}æœˆ"].iloc[0:1]
    last_month_date = report_date - relativedelta(months=1)
    last_month_data = df_final[df_final['å‘¨æœŸ'] == f"{last_month_date.month}æœˆ"].iloc[0:1]

    if not this_month_data.empty and not last_month_data.empty:
        diff_row_data = this_month_data.select_dtypes(include=np.number).iloc[0] - last_month_data.select_dtypes(include=np.number).iloc[0]
        wow_row_data = diff_row_data / last_month_data.select_dtypes(include=np.number).iloc[0].replace(0, np.nan)

        diff_row = pd.DataFrame([{'å‘¨æœŸ': 'æ•°å·®', 'å§‹æ—¥-æœ«æœŸ': 'æœ¬æœˆ-ä¸Šæœˆ'}])
        wow_row = pd.DataFrame([{'å‘¨æœŸ': 'ç¯æ¯”', 'å§‹æ—¥-æœ«æœŸ': 'æœ¬æœˆ/ä¸Šæœˆ-1'}])

        for col in diff_row_data.index:
             if col in PRODUCT_REPORT_COLS: diff_row[col] = diff_row_data[col]
        for col in wow_row_data.index:
             if col in PRODUCT_REPORT_COLS: wow_row[col] = wow_row_data[col]

        df_final = pd.concat([df_final, diff_row, wow_row], ignore_index=True)

    for col in PRODUCT_MONTHLY_REPORT_COLS:
        if col not in df_final.columns: df_final[col] = ''
    df_final = df_final[PRODUCT_MONTHLY_REPORT_COLS].fillna('')
    write_sheet_with_metadata(df_final, 'äº§å“-æœˆæŠ¥æ€»æ±‡', writer, 'æœˆæŠ¥', EXCHANGE_RATE)


def generate_summary_sheet(df_source, group_key, final_cols, time_period_str, sheet_name, writer, all_historical_data=None, df_recharge_dept=None, df_recharge_total=None):
    if df_source.empty: return
    df_report = df_source.copy()

    if time_period_str == 'æ—¥æŠ¥':
        if group_key: df_summary = df_report.groupby(group_key, as_index=False).sum(numeric_only=True)
        else: df_summary = pd.DataFrame(df_report.sum(numeric_only=True)).T; df_summary['äº§å“'] = 'aa-å·´åŸºæ–¯å¦ï¼ˆEpiWinï¼‰'
        df_summary = calculate_metrics(df_summary)
        report_date = df_report['æ—¥æœŸ'].iloc[0]
        group_cols = group_key if group_key else []
        for days in [7, 15, 30]:
            start_date = report_date - timedelta(days=days-1)
            window_df = all_historical_data[(all_historical_data['æ—¥æœŸ'] >= start_date) & (all_historical_data['æ—¥æœŸ'] <= report_date)]
            if not window_df.empty:
                if not group_cols:
                    total_recharge = window_df['å……å€¼é‡‘é¢(U)'].sum(); total_new_users = window_df['æ–°å¢ç”¨æˆ·æ•°'].sum()
                    df_summary[f'LTV-{days}å¤©'] = total_recharge / total_new_users if total_new_users else 0
                else:
                    ltv_agg = window_df.groupby(group_cols)[['å……å€¼é‡‘é¢(U)', 'æ–°å¢ç”¨æˆ·æ•°']].sum().reset_index()
                    ltv_agg[f'LTV-{days}å¤©'] = ltv_agg['å……å€¼é‡‘é¢(U)'] / ltv_agg['æ–°å¢ç”¨æˆ·æ•°'].replace(0, np.nan)
                    if not ltv_agg.empty: df_summary = pd.merge(df_summary, ltv_agg[group_cols + [f'LTV-{days}å¤©']], on=group_cols, how='left')
            else: df_summary[f'LTV-{days}å¤©'] = 0

        # å°†å¤å……ç‡åˆå¹¶åˆ°æ—¥æŠ¥
        if not group_key and df_recharge_total is not None and not df_recharge_total.empty:
            for rate_col, value in df_recharge_total.items():
                df_summary[rate_col] = value
        elif 'éƒ¨é—¨' in group_key and df_recharge_dept is not None and not df_recharge_dept.empty:
             # [ä¿®æ­£] æ¸ é“æŠ¥è¡¨ä¹Ÿåº”è¯¥ç»§æ‰¿å…¶æ‰€å±éƒ¨é—¨çš„å¤å……ç‡
            if 'æ¸ é“æ¥æº' in group_key:
                df_summary = pd.merge(df_summary, df_recharge_dept, on='éƒ¨é—¨', how='left')
            else:
                df_summary = pd.merge(df_summary, df_recharge_dept, on='éƒ¨é—¨', how='left')

        df_summary['æ—¥æœŸ'] = report_date
    else: # å‘¨æŠ¥/æœˆæŠ¥æ€»æ±‡ (ä»…ç”¨äºéƒ¨é—¨)
        grouping_keys = group_key + ['æ—¥æœŸ'] if group_key else ['æ—¥æœŸ']
        if 'äº§å“' in group_key: grouping_keys = ['æ—¥æœŸ']
        df_summary = df_report.groupby(grouping_keys, as_index=False).sum(numeric_only=True)
        if 'äº§å“' in group_key: df_summary['äº§å“'] = 'aa-å·´åŸºæ–¯å¦ï¼ˆEpiWinï¼‰'
        df_summary = calculate_metrics(df_summary)

    for col in final_cols:
        if col not in df_summary.columns: df_summary[col] = 0

    if time_period_str != 'æ—¥æŠ¥':
        total_row = pd.DataFrame(df_summary.sum(numeric_only=True)).T; total_row = calculate_metrics(total_row)
        total_row['æ—¥æœŸ'] = "æ€»è®¡"
        if group_key: total_row[group_key[0]] = 'æ€»è®¡'
        for col in df_summary.columns:
            if col not in total_row.columns: total_row[col] = ''
        df_summary = pd.concat([df_summary.fillna(''), total_row], ignore_index=True)

    # æœ€ç»ˆæ ¼å¼åŒ–ï¼šæ‰€æœ‰æ¯”ç‡åˆ—éƒ½åº”ç”¨ç™¾åˆ†æ¯”æ ¼å¼
    for col in df_summary.columns:
        if '(%)' in col or 'ç‡' in col:
             df_summary[col] = pd.to_numeric(df_summary[col], errors='coerce').apply(lambda x: f"{x:.2%}" if pd.notna(x) else 'N/A')

    write_sheet_with_metadata(df_summary[final_cols], sheet_name, writer, time_period_str, EXCHANGE_RATE)

def main():
    print("--- æŠ¥è¡¨ç”Ÿæˆå™¨å¼€å§‹è¿è¡Œ ---")
    df_channel = read_source_file('download_æ¸ é“æŠ¥è¡¨_*.csv')
    df_ops = read_source_file('download_è¿è¥æŠ¥è¡¨_*.csv')
    df_spend = get_spend_data_from_google_sheet(SPEND_SHEET_URL)
    df_mapping = read_source_file('æ€»ä»£åç§°éƒ¨é—¨è§„å¾‹è¡¨.csv')
    df_recharge_source = read_source_file('download_é¦–å……ç”¨æˆ·åˆ†æ_*.csv')

    core_files = {'æ¸ é“æŠ¥è¡¨': df_channel, 'éƒ¨é—¨æ˜ å°„è¡¨': df_mapping}
    for name, df in core_files.items():
        if df is None: print(f"[è‡´å‘½é”™è¯¯] {name}ç¼ºå¤±, æ— æ³•ç»§ç»­ã€‚"); return

    for df in [df_channel, df_ops, df_spend, df_recharge_source]:
        if df is not None and 'æ—¥æœŸ' in df.columns:
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce').dt.date

    print("\n--- æ­£åœ¨åˆå¹¶æ‰€æœ‰å†å²æ•°æ® ---")
    df_merged = df_channel.copy()
    if df_ops is not None and 'è€ç©å®¶æ—¥æ´»' in df_ops.columns:
        # [ä¿®æ­£] é€šè¿‡æ˜ç¡®æŒ‡å®šsuffixesæ¥è§£å†³åˆ—åå†²çªé—®é¢˜
        df_merged = pd.merge(
            df_merged,
            df_ops[['æ—¥æœŸ', 'è€ç©å®¶æ—¥æ´»']].drop_duplicates(subset=['æ—¥æœŸ']),
            on='æ—¥æœŸ',
            how='left',
            suffixes=('_old', '') # æ¥è‡ªå³è¡¨(df_ops)çš„åˆ—ä¿ç•™åŸåï¼Œå·¦è¡¨çš„ä¼šè¢«é‡å‘½å
        )
        if 'è€ç©å®¶æ—¥æ´»_old' in df_merged.columns:
            df_merged = df_merged.drop(columns=['è€ç©å®¶æ—¥æ´»_old']) # èˆå¼ƒä¸å†éœ€è¦çš„æ—§åˆ—

        # ç°åœ¨å¯ä»¥å®‰å…¨åœ°æ£€æŸ¥æ•°æ®ç¼ºå¤±
        missing_dau_dates = df_merged[df_merged['è€ç©å®¶æ—¥æ´»'].isna()]['æ—¥æœŸ'].unique()
        if len(missing_dau_dates) > 0:
            print("\n" + "="*20 + " [æ•°æ®è­¦å‘Š] " + "="*20)
            print("  'è¿è¥æŠ¥è¡¨' ä¸­ç¼ºå¤±ä»¥ä¸‹æ—¥æœŸçš„ 'è€ç©å®¶æ—¥æ´»' æ•°æ®ï¼š")
            sorted_dates = sorted([d for d in missing_dau_dates if pd.notna(d)])
            for d in sorted_dates[:5]:
                print(f"   - {d.strftime('%Y-%m-%d')}")
            if len(sorted_dates) > 5:
                print(f"   - ... and {len(sorted_dates) - 5} more dates")
            print("  è¿™å°†å¯¼è‡´ä¾èµ– 'è€ç©å®¶æ—¥æ´»' çš„æ¯”ç‡ï¼ˆå¦‚'è€ç”¨æˆ·ä»˜è´¹ç‡'ï¼‰è®¡ç®—ä¸å‡†ç¡®æˆ–ä¸º0ã€‚")
            print("  è¯·æ£€æŸ¥å¹¶ç¡®ä¿'è¿è¥æŠ¥è¡¨'çš„æ•°æ®æ˜¯å®Œæ•´ä¸”æœ€æ–°çš„ã€‚")
            print("="*65 + "\n")

    if df_spend is not None:
        df_merged = pd.merge(df_merged, df_spend.drop_duplicates(subset=['æ—¥æœŸ']), on='æ—¥æœŸ', how='left')
    df_merged.fillna(0, inplace=True)

    df_merged['äº§å“'] = 'aa-å·´åŸºæ–¯å¦ï¼ˆEpiWinï¼‰'

    cols_to_numerify = ['æ–°å¢ç”¨æˆ·æ•°', 'å……å€¼äººæ•°', 'å……å€¼é‡‘é¢', 'æç°é‡‘é¢', 'é¦–å­˜äººæ•°', 'é¦–å­˜å……å€¼é‡‘é¢', 'æ–°å¢ä»˜è´¹äººæ•°', 'æ–°å¢å……å€¼é‡‘é¢', 'è€ç”¨æˆ·å……å€¼äººæ•°', 'è€ç”¨æˆ·å……å€¼é‡‘é¢', 'è€ç”¨æˆ·æç°é‡‘é¢', 'è€ç©å®¶æ—¥æ´»']
    for col in cols_to_numerify:
        if col in df_merged.columns: df_merged[col] = pd.to_numeric(df_merged[col], errors='coerce').fillna(0)
    df_merged['éƒ¨é—¨'] = df_merged['æ¸ é“æ¥æº'].apply(lambda x: next((dept for keyword, dept in zip(df_mapping.iloc[:, 1], df_mapping.iloc[:, 0]) if keyword in str(x)), 'unknown'))
    all_historical_data = calculate_metrics(df_merged)

    all_historical_data.dropna(subset=['æ—¥æœŸ'], inplace=True)
    all_historical_data['æ—¥æœŸ'] = pd.to_datetime(all_historical_data['æ—¥æœŸ'], errors='coerce').dt.date

    report_date = pd.to_datetime(OVERRIDE_DATE).date() if OVERRIDE_DATE and str(OVERRIDE_DATE).strip() != "" else all_historical_data['æ—¥æœŸ'].max()
    print(f"\n[æ ¸å¿ƒ] å°†ä¸ºæ—¥æœŸ: {report_date} ç”ŸæˆæŠ¥è¡¨")

    df_recharge_dept, df_recharge_total = pd.DataFrame(), pd.Series(dtype=float)
    if df_recharge_source is not None:
         df_recharge_dept, df_recharge_total = read_and_process_recharge_data(df_recharge_source, df_mapping, report_date)

    df_daily = all_historical_data[all_historical_data['æ—¥æœŸ'] == report_date].copy()
    start_of_week = report_date - timedelta(days=report_date.weekday())
    df_weekly = all_historical_data[(all_historical_data['æ—¥æœŸ'] >= start_of_week) & (all_historical_data['æ—¥æœŸ'] <= report_date)].copy()
    start_of_month = report_date.replace(day=1)
    df_monthly = all_historical_data[(all_historical_data['æ—¥æœŸ'] >= start_of_month) & (all_historical_data['æ—¥æœŸ'] <= report_date)].copy()

    with pd.ExcelWriter(OUTPUT_FILENAME, engine='xlsxwriter') as writer:
        # --- æ—¥æŠ¥ ---
        generate_summary_sheet(df_daily, [], PRODUCT_COLS, 'æ—¥æŠ¥', 'äº§å“-æ—¥æŠ¥', writer, all_historical_data=all_historical_data, df_recharge_total=df_recharge_total)
        generate_summary_sheet(df_daily, ['éƒ¨é—¨'], DEPT_COLS, 'æ—¥æŠ¥', 'éƒ¨é—¨-æ—¥æŠ¥', writer, all_historical_data=all_historical_data, df_recharge_dept=df_recharge_dept)
        generate_summary_sheet(df_daily, ['éƒ¨é—¨', 'æ¸ é“æ¥æº'], CHANNEL_COLS, 'æ—¥æŠ¥', 'æ¸ é“-æ—¥æŠ¥', writer, all_historical_data=all_historical_data, df_recharge_dept=df_recharge_dept)

        # --- éƒ¨é—¨å‘¨æŠ¥/æœˆæŠ¥æ€»æ±‡ ---
        generate_summary_sheet(df_weekly, ['éƒ¨é—¨'], DEPT_COLS, 'å‘¨æŠ¥æ€»æ±‡', 'éƒ¨é—¨-å‘¨æŠ¥æ€»æ±‡', writer)
        generate_summary_sheet(df_monthly, ['éƒ¨é—¨'], DEPT_COLS, 'æœˆæŠ¥æ€»æ±‡', 'éƒ¨é—¨-æœˆæŠ¥æ€»æ±‡', writer)

        # --- æ–°çš„äº§å“å‘¨æŠ¥/æœˆæŠ¥ ---
        generate_product_weekly_report(all_historical_data, report_date, writer, df_recharge_source)
        generate_product_monthly_report(all_historical_data, report_date, writer, df_recharge_source)

    print("\n" + "="*50); print(f"ğŸ‰ æŠ¥è¡¨ç”ŸæˆæˆåŠŸï¼è¯·æŸ¥çœ‹: {OUTPUT_FILENAME}"); print("="*50)

if __name__ == '__main__':
    main()

